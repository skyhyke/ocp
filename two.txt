1. Configure authentication on your OpenShift instance so that:

a) The password file is /etc/origin/openshift-password

b) The user randy exists with password redhat

c) The user sam exists with password redhat

d) Both users must be able to authenticate to the Openshift Instance via CLI and on the Web Console at https://master.lab.example.com:8443

e) No user should be able to create any project

[Ref: Cluster Administration – 4.2.2]

# htpasswd -b /etc/origin/openshift-passwd sam redhat

# htpasswd -b /etc/origin/openshift-passwd randy redhat

# oc get clusterrolebinding | grep self

# oc adm policy remove-cluster-role-from-group self-provosioner system:authenticated system:authenticated:oauth

--------------------------------------------------------------------------------------------------------------------

2. a) Configure persistent Volume using the given template in the path http://rhgls.lab9.example.com/materials

b) /OSE_registry is the NFS available on workstation.lab.example.com

c) Associate the share /OSE_registry to the registry running within your OpenShift Enterprice so that it will be used as permanent storage registry.

d) Use exam-registry-volume for the volume name and exam-registry-claim for the claim name.

[Ref: Installation and configuration 22.2.2]

# vim pv.yaml apiVersion: v1

kind: PersistentVolume metadata:

 name: exam-registry-volume spec:

capacity: storage: 15Gi accessModes:

- ReadWriteOnce

nfs:

path: /OSE_registry

server: storage.area.example.com

persistentVolumeReclaimPolicy: Recycle

claimRef:

name: exam-registry-claim

namespace: default

# oc get pods

# oc create -f pv.yaml

# oc get pv

# oc describe dc docker-registry | grep -A2 Volume

# oc set volume dc docker-registry -t pvc --add --overwrite --name=registry-storage --claim-name=exam-registry-claim --claim-mode=ReadWriteOnce --claim-size=5Gi

# oc get pods

# oc describe dc docker-registry | grep -A2 Volume

--------------------------------------------------------------------------------------------------------------------

3. a) Create new projects farm, ditto, space, sample and rome

b) Description should be “This is an EX280 project on OpenShift v3”

c) Make randy the admin of rome and space.

d) Provide sam the view permission to rome.

e) Make sam the admin of farm, ditto & sample.

# oc new-project --description=“This is an EX280 project on OpenShift v3” rome

# oc new-project --description=“This is an EX280 project on OpenShift v3” space

# oc new-project --description=“This is an EX280 project on OpenShift v3” farm

# oc new-project --description=“This is an EX280 project on OpenShift v3” sample

# oc new-project --description=“This is an EX280 project on OpenShift v3” ditto

# oc get projects

# oc policy add-role-to-user admin randy -n rome

# oc policy add-role-to-user admin randy -n space

# oc policy add-role-to-user view sam -n rome


# oc policy add-role-to-user admin sam -n ditto

# oc policy add-role-to-user admin sam -n sample

# oc describe policybinding -n <projectname>

--------------------------------------------------------------------------------------------------------------------------------


4. a) Use S2I to create an application in the project rome.

b) Use the GIT repository http://workstation.lab.example.com/php-helloworld as the source image for the application

c) The docker image for the application is openshift/rhlsc-ruby55-rhel7.

d) Once deployed the application must be reachable at the following address http://mordor.cloudapps.lab.example.com

e) Replace the word PLACEHOLDER in the app.rb file with the text provided in http://rgls9.area.example.com/openshit/mordor.txt

f) Trigger a rebuild so that when browsing http://mordor.cloudapps.example.com it will display the new text.

# oc project rome

# oc new-app registry.area.example.com:5000/openshift/rhlsc-ruby55-rhel7~http://workstation.lab.example.com/php-helloworld

# oc get pods

# oc get svc

# oc expose svc mordor --hostname=mordor.rome.area.example.com

# oc get route

# curl http://mordor.cloudapps.lab.example.com

# git clone http://workstation.lab.example.com/php-helloworld

# cd php-helloworld

# vim app.rb [Replace the work PLACEHOLDER to the one in the txt given url]

# git add .


# git commit -n “Updated Version”

# git push origin master

# oc start-build modor

# oc get pods

# curl http://mordor.cloudapps.lab.example.com


--------------------------------------------------------------------------------------------------------------------




5. a) Using the example files from the wordpress directory under http://rhgls.lab9.example.com/materials/openshift/origin/examples create a Wordpress application in the farm project

b) For permenant storage use the shares /OSE_wordpress & /OSE-mysql.

c) Use the wordpress.tar file to create the wordpress application provided in the url http://rgls9.area.example.com/wordpress.tar.

d) Deploy the mysql application from openshift3/mysql-55-rhel7.

e) Once the application is deployed make it reachable from http://wordpress.cloudapps.lab.example.com.

f) Complete the wordpress installation by setting "sam" as the user account with password

“ablerate" and mail id is "sam@area.example.com"

g) Create a blog EX280 Blog and Create your first post with title:Carpe dlem.quam minimum credula postero(The text in the post does not matter)

# showmount -e storage.area.example.com

# vim wordpress-pv.yml

apiVersion: v1

kind: PersistentVolume

metadata:

name: wordpress

spec:

capacity:

storage: 5Gi

accessModes:

- ReadWriteMany

nfs:

path: /OSE_wordpress

server: storage.area.example.com

persistentVolumeReclaimPolicy: Recycle

# vim mysql-pv.yml apiVersion: v1

kind: PersistentVolume metadata:

 name: mysql spec: capacity:

 storage: 5Gi accessModes:

- ReadWriteMany nfs:

path: /OSE_mysql

 server: storage.area.example.com persistentVolumeReclaimPolicy: Recycle

# oc create -f wordpress-pv.yml

# oc create -f mysql-pv.yml

# wget http://rgls9.area.example.com/wordpress.tar

# docker load -i wordpress.tar

# docker images

# docker tag <ID> registry.area.example.com:5000/openshift/wordpress

# docker images

# docker push registry.area.example.com:5000/openshift/wordpress

# oc new-app --docker-image= registry.area.example.com:5000/openshift/wordpress -- name=wordpress

# oc get pods

# oc create serviceaccount useroot

# oc adm policy add-scc-to-user anyuid -z useroot

# oc edit dc wordpress

serviceAccountName: useroot

# oc get pods

# oc new-app --docker-image= registry.area.example.com:5000/openshift3/mysql-55-rhel7 -- name=mysql --insecure-registry -e MYSQL_USER=wp_user -e MYSQL_PASSWORD=wp_pass -e MYSQL_DATABASE=wpdb

# oc get pods

# oc describe dc wordpress | grep -A2 Volume

# oc set volume dc wordpress -t pvc --add --overwrite --name=wordpress-volume-1 --claim-name=claim-wp --claim-size=5Gi --claim-mode=ReadWriteMany

# oc get pods

# oc describe dc wordpress | grep -A2 Volume

# oc set volume dc wordpress -t pvc --add --overwrite --name=wordpress-volume-1 --claim-name=claim-wp --claim-size=5Gi --claim-mode=ReadWriteMany

# oc get pods

# oc get pv

# oc get pvc

# oc get pods

# oc get svc

# oc expose svc wordpress -hostname=wordpress.cloudapps.lab.example.com


Note: Got to firefox and provide this URL it should take you to wordpress installation and continue the remaining steps.

----------------------------------------------------------------------------------------------------------------------




6. a) Create a secure route for the hostname https://greeter.cloudapps.lab.example.com in the project space for the application greeter using the docker image openshift/hello-openshift.

b) The certificates can be generated using the generate-cert.sh in the link http://rgls9.area.example.com/generate.sh.


# oc project space


# oc new-app –docker-image=registry.area.example.com:5000/openshift/hello-openshift --insecure-registry --name=greeter

# oc get pods

# oc get svc

# wget http://rgls9.area.example.com/generate.sh

# sh generate.sh couldapps.example.com

# oc create route edge --service=greeter --cert=greeter.cloudapps.lab.example.com.crt -- key=greeter.cloudapps.lab.example.com.key --hostname= greeter.cloudapps.lab.example.com

# oc get route

# curl -k https:// greeter.space.devclouds.area.example.com


--------------------------------------------------------------------------------------------------------------------



7. a) Create an application in project ditto with the template in the path .

b) In the template change postgressql version from 9.5 to 9.2.

c) The template must be made available for all users across project.

d) Create a docker image with the gogs.tar in the given path http://rhgls.lab9.example.com/materials.

e) The docker image stream tag must be 0.9.97.

f) Deploy the application from the template and make it reachable through the hostname gogs.cloudapps.lab.example.com.

g) Create a repository and add the text “Carpe dlem.quam minimum credula postero” to the

README.md file (create README.md file if it is not available) .

[For LAB practice on this one, do page 378 – 384]

# oc project ditto

# wget https://rgls9.area.example.com/materials/gogs.tar

# docker load -i gogs.tar

# docker images

# docker tag <ID> registry.area.example.com:5000/openshiftdemos/gogs:0.9.97

# docker images

# docker push registry.area.example.com:5000/openshiftdemos/gogs:0.9.97

# vim gogs-js.json

Change the version of postgresql from 9.5 to 9.2 Change docker.io to registry.area.example.com:5000

# oc create -f gogs-js.json -n openshift


• Deploy the application using the template in Ditto Project in GUI with the given hostname.

• Once deployed ensure two pods are running (gogs and postgresql).

• Create the repository and modify the README.md file with the gven text.

-----------------------------------------------------------------------------------------------------------------------


8. Scale the greeter to the count of 5.

# oc project space

# oc scale –replicas=5 dc greeter

-----------------------------------------------------------------------------------------------------------------------


9. a) Configure quotas and limits for project sample so that:

b) The ResourceQuota resouce is named ex280-quota

The amount of memory consumed across all containers may not exceed 1Gi

The Total amount of CPU usage consumed across all containers may not exceed 2 Kubernetes compute untis.

The maximum number of replication controllers does not exceed 3

The maximum number of services of pods does not exceed 12

The maximum number of services does not exceed 6

c) The LimitRange resource is named ex280-limits


The amount of memory consumed by a single pod is between 5Mi and 300Mi The amount of cpu consumed by a single pod is between 10m and 500m

The amount of cpu consumed by a single container is between 10m and 500m with a default request value of 100m

[Ref: Cluster Administration – 13.3 and 15.1]

# vim quota.yaml apiVersion: v1 kind: ResourceQuota metadata:

 name: ex280-quota spec:

hard: pods: "3" limits.cpu: "2" limits.memory: 1Gi replicationcontrollers: 3 services: 6



# vim limits.yaml apiVersion: "v1" kind: "LimitRange" metadata:

 name: "ex-280-limits" spec:

limits:

- type: "Pod" max:

cpu: "500m"

 memory: "300Mi" min:

cpu: "10m"

memory: "5Mi"

- type: "Container" max:

cpu: "500m"

 memory: "300Mi" min:

cpu: "10m"

 memory: "5Mi" defaultRequest: cpu: "100m"
 


# oc create -f quota.yaml


# oc create -f limits.yaml

# oc describe quota ex280-quota

# oc describe limitrange ex280-limits


--------------------------------------------------------------------------------------------------------------------



10. Install Metrics with the template and provided options

The template path is /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-metrics.yml and the hosts file path is /root/hosts

openshift_metrics_image_version=v3.5

openshift_metrics_heapster_requests_memory=300M openshift_metrics_hawkular_requests_memory=750M openshift_metrics_cassandra_requests_memory=750M openshift_metrics_cassandra_storage_type=pv openshift_metrics_cassandra_pvc_size=5Gi openshift_metrics_cassandra_pvc_prefix=metrics openshift_metrics_install_metrics=True

[Ref: Installation and configuration – 31.5.1]


# oc project openshift-infra

# vim metrics.yaml apiVersion: v1

kind: PersistentVolume metadata:

 name: metricspv spec: capacity:

 storage: 5Gi accessModes:

- ReadWriteOnce

nfs:

path: /OSE_metrics

server: storage.area.example.com

persistentVolumeReclaimPolicy: Recycle

# oc create -f metrics.yaml

# ansible-playbook -i /root/hosts /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-metrics.yml

-e openshift_metrics_image_prefix=registry.area.example.com:5000/openshift3/ose--e openshift_metrics_image_version=v3.5

-e openshift_metrics_heapster_requests_memory=300M -e openshift_metrics_hawkular_requests_memory=750M -e openshift_metrics_cassandra_requests_memory=750M -e openshift_metrics_cassandra_storage_type=pv

-e openshift_metrics_cassandra_pvc_size=5Gi

-e openshift_metrics_cassandra_pvc_prefix=metrics -e openshift_metrics_install_metrics=True

# oc get pods

# oc get pods -w

# oc get route


Launch the url in firefox and accept the certificate to start the hawkular service.

https://hawkular-metrics.cloudapps.lab.example.com/hawkular/metrics
